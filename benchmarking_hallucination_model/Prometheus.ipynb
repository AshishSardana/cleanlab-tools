{"cells":[{"cell_type":"markdown","metadata":{"id":"LvDkrXSDetJW"},"source":["# Prometheus\n","\n","\n"]},{"cell_type":"markdown","metadata":{"id":"92S7AJpY2B97"},"source":["You'd need a GPU with atleast 32GB RAM in order to load the smallest Prometheus model i.e. 7B FP32.\n","\n","You'd need 4 GPUs with atleast 32 GB RAM each to load the bigger Prometheus model i.e. 8x7B MoE."]},{"cell_type":"markdown","metadata":{"id":"0A7DSD1IJpa0"},"source":["Install dependencies"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"RAUpvHWPFIlR"},"outputs":[],"source":["%pip install datasets\n","%pip install prometheus-eval vllm triton"]},{"cell_type":"markdown","source":["Load model"],"metadata":{"id":"45jEfimGCYz5"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"-BAwGT8tJSvm","colab":{"referenced_widgets":["95cb14b1f061419dbfe80c215c2e6c49"]},"outputId":"1cc4f80e-830e-4409-c2e1-7441bf6a003f"},"outputs":[{"name":"stdout","output_type":"stream","text":["INFO 12-16 04:07:31 config.py:1020] Defaulting to use mp for distributed inference\n","INFO 12-16 04:07:31 llm_engine.py:249] Initializing an LLM engine (v0.6.4.post1) with config: model='prometheus-eval/prometheus-8x7b-v2.0', speculative_config=None, tokenizer='prometheus-eval/prometheus-8x7b-v2.0', skip_tokenizer_init=False, tokenizer_mode=auto, revision=None, override_neuron_config=None, tokenizer_revision=None, trust_remote_code=False, dtype=torch.bfloat16, max_seq_len=32768, download_dir=None, load_format=LoadFormat.AUTO, tensor_parallel_size=4, pipeline_parallel_size=1, disable_custom_all_reduce=False, quantization=None, enforce_eager=False, kv_cache_dtype=auto, quantization_param_path=None, device_config=cuda, decoding_config=DecodingConfig(guided_decoding_backend='outlines'), observability_config=ObservabilityConfig(otlp_traces_endpoint=None, collect_model_forward_time=False, collect_model_execute_time=False), seed=0, served_model_name=prometheus-eval/prometheus-8x7b-v2.0, num_scheduler_steps=1, chunked_prefill_enabled=False multi_step_stream_outputs=True, enable_prefix_caching=False, use_async_output_proc=True, use_cached_outputs=False, chat_template_text_format=string, mm_processor_kwargs=None, pooler_config=None)\n","WARNING 12-16 04:07:31 multiproc_gpu_executor.py:56] Reducing Torch parallelism from 24 threads to 1 to avoid unnecessary CPU contention. Set OMP_NUM_THREADS in the external environment to tune this value as needed.\n","INFO 12-16 04:07:31 custom_cache_manager.py:17] Setting Triton cache manager to: vllm.triton_utils.custom_cache_manager:CustomCacheManager\n","INFO 12-16 04:07:31 selector.py:135] Using Flash Attention backend.\n","\u001b[1;36m(VllmWorkerProcess pid=4047)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4048)\u001b[0;0m INFO 12-16 04:07:33 selector.py:135] Using Flash Attention backend.\n","INFO 12-16 04:07:33 selector.py:135] Using Flash Attention backend.\n","INFO 12-16 04:07:33 selector.py:135] Using Flash Attention backend.\n","\u001b[1;36m(VllmWorkerProcess pid=4048)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4047)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4049)\u001b[0;0m INFO 12-16 04:07:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","INFO 12-16 04:07:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","INFO 12-16 04:07:33 multiproc_worker_utils.py:215] Worker ready; awaiting tasks\n","INFO 12-16 04:07:35 utils.py:961] Found nccl from library libnccl.so.2\n","INFO 12-16 04:07:35 pynccl.py:69] vLLM is using nccl==2.21.5\n","\u001b[1;36m(VllmWorkerProcess pid=4048)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4047)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4049)\u001b[0;0m INFO 12-16 04:07:35 utils.py:961] Found nccl from library libnccl.so.2\n","INFO 12-16 04:07:35 utils.py:961] Found nccl from library libnccl.so.2\n","INFO 12-16 04:07:35 utils.py:961] Found nccl from library libnccl.so.2\n","\u001b[1;36m(VllmWorkerProcess pid=4048)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4047)\u001b[0;0m INFO 12-16 04:07:35 pynccl.py:69] vLLM is using nccl==2.21.5\n","INFO 12-16 04:07:35 pynccl.py:69] vLLM is using nccl==2.21.5\n","INFO 12-16 04:07:35 pynccl.py:69] vLLM is using nccl==2.21.5\n","INFO 12-16 04:07:37 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/asardana/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n","\u001b[1;36m(VllmWorkerProcess pid=4047)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4048)\u001b[0;0m INFO 12-16 04:07:37 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/asardana/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n","INFO 12-16 04:07:37 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/asardana/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n","INFO 12-16 04:07:37 custom_all_reduce_utils.py:242] reading GPU P2P access cache from /home/asardana/.cache/vllm/gpu_p2p_access_cache_for_0,1,2,3.json\n","INFO 12-16 04:07:37 shm_broadcast.py:236] vLLM message queue communication handle: Handle(connect_ip='127.0.0.1', local_reader_ranks=[1, 2, 3], buffer=<vllm.distributed.device_communicators.shm_broadcast.ShmRingBuffer object at 0x7f9c7a78bc90>, local_subscribe_port=56915, remote_subscribe_port=None)\n","INFO 12-16 04:07:37 model_runner.py:1072] Starting to load model prometheus-eval/prometheus-8x7b-v2.0...\n","\u001b[1;36m(VllmWorkerProcess pid=4048)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4049)\u001b[0;0m \u001b[1;36m(VllmWorkerProcess pid=4047)\u001b[0;0m INFO 12-16 04:07:37 model_runner.py:1072] Starting to load model prometheus-eval/prometheus-8x7b-v2.0...\n","INFO 12-16 04:07:37 model_runner.py:1072] Starting to load model prometheus-eval/prometheus-8x7b-v2.0...\n","INFO 12-16 04:07:37 model_runner.py:1072] Starting to load model prometheus-eval/prometheus-8x7b-v2.0...\n","\u001b[1;36m(VllmWorkerProcess pid=4049)\u001b[0;0m INFO 12-16 04:07:37 weight_utils.py:243] Using model weights format ['*.safetensors']\n","INFO 12-16 04:07:37 weight_utils.py:243] Using model weights format ['*.safetensors']\n","\u001b[1;36m(VllmWorkerProcess pid=4047)\u001b[0;0m INFO 12-16 04:07:37 weight_utils.py:243] Using model weights format ['*.safetensors']\n","\u001b[1;36m(VllmWorkerProcess pid=4048)\u001b[0;0m INFO 12-16 04:07:37 weight_utils.py:243] Using model weights format ['*.safetensors']\n"]},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"95cb14b1f061419dbfe80c215c2e6c49","version_major":2,"version_minor":0},"text/plain":["Loading safetensors checkpoint shards:   0% Completed | 0/19 [00:00<?, ?it/s]\n"]},"metadata":{},"output_type":"display_data"}],"source":["from prometheus_eval.vllm import VLLM\n","from prometheus_eval import PrometheusEval\n","\n","model_name_7b = \"prometheus-eval/prometheus-7b-v2.0\"\n","# model_7b = VLLM(model=model_name_7b)\n","\n","model_name_8x7b = \"prometheus-eval/prometheus-8x7b-v2.0\"\n","model_8x7b = VLLM(model=model_name_8x7b, tensor_parallel_size=4) # tensor_parallel_size = num_of_gpus"]},{"cell_type":"markdown","source":["### Halubench"],"metadata":{"id":"ZULcigX-CZ71"}},{"cell_type":"code","execution_count":null,"metadata":{"id":"nN9wdjuxIMhE"},"outputs":[],"source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"PatronusAI/HaluBench\")\n","data = ds[\"test\"].to_pandas()\n","\n","# store sources for subsets\n","sources = data.source_ds.unique().tolist()\n","sources.remove('RAGTruth')\n","sources.remove('halueval')"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"n3RpXhZxHhFU"},"outputs":[],"source":["import os\n","import json\n","\n","def save_to_json(filename, feedbacks, scores, source):\n","    results_dict = {}\n","\n","    # Check if the JSON file exists and load its content\n","    if os.path.exists(f'{filename}.json'):\n","        try:\n","            with open(f'{filename}.json', 'r') as f:\n","                file_content = f.read().strip()\n","                if file_content:  # Ensure the file is not empty\n","                    results_dict = json.loads(file_content)\n","        except (json.JSONDecodeError, IOError) as e:\n","            print(f\"Error loading {filename}.json: {e}. Initializing as empty.\")\n","\n","    # Add the new results under the source key\n","    results_dict[source] = [{'feedback': feedback, 'score': score} for feedback, score in zip(feedbacks, scores)]\n","\n","    # Write the updated dictionary back to the file\n","    try:\n","        with open(f'{filename}.json', 'w') as f:\n","            json.dump(results_dict, f, indent=2)\n","    except IOError as e:\n","        print(f\"Error saving {filename}.json: {e}\")"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"68kU2eO_Dclk"},"outputs":[],"source":["def run_metrics(data, sources, model, prompt_template, rubrics, filename, normalize=False):\n","    for source in sources:\n","        # extract subset\n","        subset = data[data.source_ds == source]\n","        # re-format\n","        instructions = []\n","        responses = []\n","\n","        for idx, row in subset.iterrows():\n","            # combine passage and question for instructions\n","            passage = \"Passage:\\n\" + row['passage']\n","            question = \"Question:\\n\" + row['question']\n","            instructions.append(passage + \"\\n\" + question)\n","\n","            answer = row['answer']\n","            responses.append(answer)\n","\n","        # initialise prometheus judge\n","        judge = PrometheusEval(model=model, absolute_grade_template=prompt_template)\n","        # run prometheus model\n","        feedbacks, scores = judge.absolute_grade(\n","            instructions=instructions,\n","            responses=responses,\n","            rubric=rubrics[source]\n","        )\n","\n","        if normalize:\n","            scores = [(score - 1) / 4 for score in scores]\n","        else:\n","            scores = [1 if score == 5 else 0 for score in scores]\n","        # save results to json\n","        save_to_json(filename, feedbacks, scores, source)\n","\n","        print(\"Completed evaluation on {0} dataset. Length of feedback: {1} and scores: {2}\".format(source, len(feedbacks), len(scores)))"]},{"cell_type":"markdown","metadata":{"id":"huqvoTTHuVBJ"},"source":["Original prompt template"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"6MLgUjccHiBx"},"outputs":[],"source":["prompt_template_orig = \"\"\"###Task Description:\n","An instruction (including passage and a question), a response to evaluate, and a score rubric representing a evaluation criteria are given.\n","1. Write a detailed feedback that assess the response strictly based on the given score rubric, not evaluating in general.\n","2. After writing a feedback, write a score that is an integer between 1 and 5. You should refer to the score rubric.\n","3. The output format should look as follows: \"(write a feedback for criteria) [RESULT] (an integer number between 1 and 5)\"\n","4. Please do not generate any other opening, closing, and explanations.\n","\n","###The instruction to evaluate:\n","{instruction}\n","\n","###Response to evaluate:\n","{response}\n","\n","###Score Rubrics:\n","{rubric}\n","\n","###Feedback: \"\"\""]},{"cell_type":"markdown","metadata":{"id":"Fnd_XPh2Hnqk"},"source":["Scoring rubrics for all 4 datasets of Halubench"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"LV4MTlCRcAEc"},"outputs":[],"source":["covidqa_rubric_data = \"\"\"\n","[Does the response contain only information that is explicitly present in the provided passage?]\n","Score 1: The response states information that is completely absent from or contradicts the passage. The answer includes specific numbers, facts, or claims that cannot be found in the passage.\n","Score 2: The response contains a mix of information from the passage and additional unsupported claims. While some elements are correct, the answer includes details, numbers, or relationships not present in the source passage.\n","Score 3: The response mostly aligns with the passage but includes minor additions or assumptions. The core information is accurate, but the answer occasionally includes details that aren't explicitly stated in the passage.\n","Score 4: The response closely follows the passage with rare deviations. Nearly all information can be directly traced to the source text, with only slight imprecisions in specific details.\n","Score 5: The response contains only information that is explicitly stated in the passage. All specific numbers, relationships, and technical details can be directly verified in the source text.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"hCktlJMhWN7q"},"outputs":[],"source":["pubmed_rubric_data = \"\"\"\n","[How accurately does the response reflect the evidence presented in the passage, maintaining appropriate scientific precision and avoiding unsupported claims or speculation?]\n","Score 1: The response makes claims that directly contradict the passage, fabricates unsupported information, misuses technical terminology, introduces speculative mechanisms or implications, makes absolute claims without appropriate uncertainty, or drastically mismatches the passage's level of detail. The response fails to maintain scientific integrity and cannot be considered reliable.\n","Score 2: The response contains significant misinterpretations of evidence, unsupported extrapolations beyond data scope, imprecise use of technical terminology, inclusion of speculative details, overgeneralization of findings, or substantial deviation from the passage's level of detail. While some aspects may be accurate, the response's reliability is compromised by these issues.\n","Score 3: The response generally aligns with the main findings but includes minor unsupported details, slight misinterpretations, occasional imprecise terminology, reasonable but unsupported elaborations, missing some limitations, or inconsistent detail level. While generally reliable, the response requires some scrutiny for complete accuracy.\n","Score 4: The response accurately reflects the evidence with only minor issues such as subtle extrapolations (though reasonable), rare imprecisions in technical terminology, occasional missing caveats, or slight variations in detail level. The response maintains good scientific integrity and can be considered largely reliable.\n","Score 5: The response perfectly reflects the presented evidence, maintains appropriate scientific uncertainty, uses precise technical terminology, avoids unsupported speculation, properly acknowledges limitations, and matches the passage's level of detail. The response maintains complete scientific integrity and can be fully relied upon as an accurate reflection of the passage.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"HP5lFwKPZbHA"},"outputs":[],"source":["drop_rubric_data = \"\"\"\n","[Does the response contain only information that is explicitly supported by the passage, maintaining accuracy and relevance to the specific question asked?]\n","Score 1: The response contains information that is completely unsupported by the passage, or contradicts the passage directly. This includes fabricated details, numbers, or claims that cannot be verified from the source passage.\n","Score 2: The response contains significant inaccuracies or unverified information, though some elements might align with the passage. The answer may include unsupported numerical values or claims that go beyond reasonable inference.\n","Score 3: The response shows partial alignment with the passage but includes some unverified details or questionable inferences. While core information might be accurate, there are elements that cannot be fully verified from the passage.\n","Score 4: The response closely aligns with the passage with very minimal unverified information. Any inferences made are reasonable and well-supported by the content, though there might be slight imprecisions in numerical values or specific details.\n","Score 5: The response contains only information that is explicitly stated in or can be directly verified from the passage. All numerical values, facts, and claims perfectly match the passage, and all inferences are fully supported by the content.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"0ERWATfobfXY"},"outputs":[],"source":["financebench_rubric_data = \"\"\"\n","[Does the response provide an answer that is verifiable against the provided passage, using specified formulas when given and adhering to any stated rounding requirements?]\n","Score 1: The response presents a value or explanation that cannot be verified using the information in the provided passage, showing no clear connection to the source data or specified calculation methods.\n","Score 2: The response shows some connection to the provided data or specified formulas, but contains significant errors or makes unsupported claims that deviate from what can be verified.\n","Score 3: The response generally aligns with the provided data and calculation methods but contains minor errors in computation, rounding, or reasoning.\n","Score 4: The response closely matches what can be verified from the provided data using proper methods, with only minimal deviations in precision or completeness.\n","Score 5: The response exactly matches what can be verified from the provided data, using specified formulas when given and adhering perfectly to any stated requirements for rounding or explanation.\n","\"\"\""]},{"cell_type":"code","execution_count":null,"metadata":{"id":"nhzMMtemE92K"},"outputs":[],"source":["scoring_rubrics = {\n","    'covidQA': covidqa_rubric_data,\n","    'pubmedQA': pubmed_rubric_data,\n","    'DROP': drop_rubric_data,\n","    'FinanceBench': financebench_rubric_data\n","}"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aGtz7ASgKBnk","scrolled":true,"outputId":"079e3887-ee57-4f4e-9609-bfb401b46ff0"},"outputs":[{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/prom/lib/python3.11/site-packages/prometheus_eval/judge.py:148: UserWarning: Reference answer was not provided. This may result in suboptimal grading performance. Consider providing a reference answer for best results.\n","  warnings.warn(\n","Processed prompts: 100%|███████████████████| 1000/1000 [01:10<00:00, 14.15it/s, est. speed input: 11529.24 toks/s, output: 2099.02 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|████████████████████████████| 4/4 [00:03<00:00,  1.19it/s, est. speed input: 960.75 toks/s, output: 195.25 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processed 1000/1000 instances.\n"]},{"name":"stderr","output_type":"stream","text":["Finalizing: 100%|██████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 17021.79it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Completed evaluation on DROP dataset. Length of feedback: 1000 and scores: 1000\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts:   0%|                                     | 0/1000 [00:00<?, ?it/s, est. speed input: 0.00 toks/s, output: 0.00 toks/s]"]},{"name":"stdout","output_type":"stream","text":["WARNING 12-08 01:56:11 scheduler.py:1481] Sequence group 2262 is preempted by PreemptionMode.RECOMPUTE mode because there is not enough KV cache space. This can affect the end-to-end performance. Increase gpu_memory_utilization or tensor_parallel_size to provide more KV cache memory. total_num_cumulative_preemption=1\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████| 1000/1000 [01:44<00:00,  9.61it/s, est. speed input: 10086.64 toks/s, output: 2144.07 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████| 1/1 [00:03<00:00,  3.35s/it, est. speed input: 325.77 toks/s, output: 92.78 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processed 1000/1000 instances.\n"]},{"name":"stderr","output_type":"stream","text":["Finalizing: 100%|██████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 11135.22it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Completed evaluation on pubmedQA dataset. Length of feedback: 1000 and scores: 1000\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████| 1000/1000 [02:29<00:00,  6.69it/s, est. speed input: 10967.98 toks/s, output: 1503.26 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████| 11/11 [00:10<00:00,  1.05it/s, est. speed input: 2362.07 toks/s, output: 303.02 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|█████████████████████████████| 1/1 [00:02<00:00,  2.19s/it, est. speed input: 557.94 toks/s, output: 91.92 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processed 1000/1000 instances.\n"]},{"name":"stderr","output_type":"stream","text":["Finalizing: 100%|██████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 13176.50it/s]\n"]},{"name":"stdout","output_type":"stream","text":["Completed evaluation on FinanceBench dataset. Length of feedback: 1000 and scores: 1000\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|████████████████████| 1000/1000 [05:10<00:00,  3.22it/s, est. speed input: 16102.00 toks/s, output: 454.69 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 1/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|████████████████████████| 64/64 [00:23<00:00,  2.78it/s, est. speed input: 15923.47 toks/s, output: 372.20 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 2/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|███████████████████████████| 5/5 [00:03<00:00,  1.26it/s, est. speed input: 8072.07 toks/s, output: 148.62 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Retrying failed batches: Attempt 3/10\n"]},{"name":"stderr","output_type":"stream","text":["Processed prompts: 100%|████████████████████████████| 1/1 [00:01<00:00,  1.69s/it, est. speed input: 4749.96 toks/s, output: 74.04 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processed 1000/1000 instances.\n"]},{"name":"stderr","output_type":"stream","text":["Finalizing: 100%|██████████████████████████████████████████████████████████████████████████████████| 1000/1000 [00:00<00:00, 17234.48it/s]"]},{"name":"stdout","output_type":"stream","text":["Completed evaluation on covidQA dataset. Length of feedback: 1000 and scores: 1000\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["filename = 'prometheus_8x7b_orig_prompt_template'\n","normalize = True\n","\n","run_metrics(data, sources, model_8x7b, prompt_template_orig, scoring_rubrics, filename, normalize=normalize)"]},{"cell_type":"markdown","source":["### ELI5"],"metadata":{"id":"EKBCPmHiCobQ"}},{"cell_type":"code","execution_count":null,"metadata":{"colab":{"referenced_widgets":["67ac0c3c1873426b8439770931f049ea","203324460ef54cd4aaa5d0780b6d9fbf","d35b5f7a58a84b64baa9d627c0d61af3","0a282499565042c8b383e9978a1c397a","2a5183a9822b42328ffdc130fa20cf0f"]},"id":"ewav34daCMTM","outputId":"d55e5737-3091-4bad-b3d8-f0a0d8580bda"},"outputs":[{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"67ac0c3c1873426b8439770931f049ea","version_major":2,"version_minor":0},"text/plain":["README.md:   0%|          | 0.00/480 [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"203324460ef54cd4aaa5d0780b6d9fbf","version_major":2,"version_minor":0},"text/plain":["test-00000-of-00001.parquet:   0%|          | 0.00/48.6k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"d35b5f7a58a84b64baa9d627c0d61af3","version_major":2,"version_minor":0},"text/plain":["train-00000-of-00001.parquet:   0%|          | 0.00/74.9k [00:00<?, ?B/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"0a282499565042c8b383e9978a1c397a","version_major":2,"version_minor":0},"text/plain":["Generating test split:   0%|          | 0/56 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"data":{"application/vnd.jupyter.widget-view+json":{"model_id":"2a5183a9822b42328ffdc130fa20cf0f","version_major":2,"version_minor":0},"text/plain":["Generating train split:   0%|          | 0/100 [00:00<?, ? examples/s]"]},"metadata":{},"output_type":"display_data"},{"name":"stderr","output_type":"stream","text":["/opt/conda/envs/prom/lib/python3.11/site-packages/prometheus_eval/judge.py:148: UserWarning: Reference answer was not provided. This may result in suboptimal grading performance. Consider providing a reference answer for best results.\n","  warnings.warn(\n","Processed prompts: 100%|████████████████████████████████████████████████████████████████| 100/100 [00:15<00:00,  6.53it/s, est. speed input: 4782.52 toks/s, output: 1417.63 toks/s]\n"]},{"name":"stdout","output_type":"stream","text":["Processed 100/100 instances.\n"]},{"name":"stderr","output_type":"stream","text":["Finalizing: 100%|██████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████████| 100/100 [00:00<00:00, 10805.33it/s]"]},{"name":"stdout","output_type":"stream","text":["Completed evaluation on eli5 dataset. Length of feedback: 100 and scores: 100\n"]},{"name":"stderr","output_type":"stream","text":["\n"]}],"source":["from datasets import load_dataset\n","\n","ds = load_dataset(\"explodinggradients/ELI5\")\n","data = ds[\"train\"].to_pandas()\n","\n","def run_metrics(data, model, prompt_template, rubrics, filename, normalize=False):\n","    # re-format\n","    instructions = []\n","    responses = []\n","\n","    for idx, row in data.iterrows():\n","        # combine passage and question for instructions\n","        passage = \"Passage:\\n\" + row['reference']\n","        question = \"Question:\\n\" + row['user_input']\n","        instructions.append(passage + \"\\n\" + question)\n","\n","        answer = row['response']\n","        responses.append(answer)\n","\n","    # initialise prometheus judge\n","    judge = PrometheusEval(model=model, absolute_grade_template=prompt_template)\n","    # run prometheus model\n","    feedbacks, scores = judge.absolute_grade(\n","        instructions=instructions,\n","        responses=responses,\n","        rubric=rubrics\n","    )\n","\n","    if normalize:\n","        scores = [(score - 1) / 4 for score in scores]\n","    else:\n","        scores = [1 if score == 5 else 0 for score in scores]\n","    # save results to json\n","    save_to_json(filename, feedbacks, scores, 'eli5')\n","\n","    print(\"Completed evaluation on {0} dataset. Length of feedback: {1} and scores: {2}\".format('eli5', len(feedbacks), len(scores)))\n","\n","eli5_rubric_data = \"\"\"\n","[Does the response contain only information that is explicitly supported by the passage, maintaining accuracy and relevance to the specific question asked?]\n","Score 1: The response contains information that is completely unsupported by the passage, or contradicts the passage directly. This includes fabricated details, numbers, or claims that cannot be verified from the source passage.\n","Score 2: The response contains significant inaccuracies or unverified information, though some elements might align with the passage. The answer may include unsupported numerical values or claims that go beyond reasonable inference.\n","Score 3: The response shows partial alignment with the passage but includes some unverified details or questionable inferences. While core information might be accurate, there are elements that cannot be fully verified from the passage.\n","Score 4: The response closely aligns with the passage with very minimal unverified information. Any inferences made are reasonable and well-supported by the content, though there might be slight imprecisions in numerical values or specific details.\n","Score 5: The response contains only information that is explicitly stated in or can be directly verified from the passage. All numerical values, facts, and claims perfectly match the passage, and all inferences are fully supported by the content.\n","\"\"\"\n","\n","filename = 'prometheus_8x7b_orig_prompt_template'\n","normalize = True\n","\n","run_metrics(data, model_8x7b, prompt_template_orig, eli5_rubric_data, filename, normalize=normalize)"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3 (ipykernel)","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.10"}},"nbformat":4,"nbformat_minor":0}