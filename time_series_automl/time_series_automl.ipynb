{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "4c0588ac-64ad-4958-9ddc-3d2c4dfdba41",
   "metadata": {},
   "source": [
    "## Modelling time series data in a tabular format using featurization and improving accuracy using AutoML"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "84186538-9eb7-449e-8c5d-25694eee9b38",
   "metadata": {},
   "source": [
    "This notebook delves into enhancing the process of forecasting average daily energy consumption levels by transforming a time series dataset into a tabular format using open-source libraries. We explore the application of a multiclass classification model and leverage AutoML with cleanlab to significantly boost our out-of-sample accuracy.\n",
    "\n",
    "At a high level we will:\n",
    "\n",
    "- Establish a baseline accuracy by fitting a Prophet forecasting model on our time series data\n",
    "- Convert our time series data into a tabular format by using open-source featurization libraries and then will show that can outperform prophet with a multiclass classification approach.\n",
    "- Use cleanlabâ€™s AutoML platform for multiclass classification to **improve our out-of-sample accuracy for our predictions by ~8%** compared to both approaches tried so fa**r.**"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "35ced8a9-59b9-405a-a578-d533cc89f5f6",
   "metadata": {},
   "source": [
    "## Initialize time series data for Prophet"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6c561683-199a-48ee-97ec-95415fcd7e35",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "pd.options.mode.chained_assignment = None \n",
    "\n",
    "data = pd.read_csv('PJME_hourly.csv', parse_dates=['Datetime'], index_col='Datetime')\n",
    "\n",
    "# Assuming pjme_data is loaded as before\n",
    "daily_data = data.resample('D').mean() \n",
    "\n",
    "# Prepare data for Prophet\n",
    "daily_data.reset_index(inplace=True)\n",
    "daily_data.columns = ['ds', 'y']"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7a85f6a2-fb50-4234-bca8-af54491acfa5",
   "metadata": {},
   "source": [
    "## Initialize time series data for featurization into a tabular format"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0cd719a8-62a2-45ed-93ba-a9b7288459bb",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "# Reset the datetime\n",
    "data[\"Datetime\"] = data.index\n",
    "data = data.reset_index(drop=True)\n",
    "\n",
    "# Create copy for multiclass data \n",
    "df = data.copy()\n",
    "\n",
    "# Convert the datetime column\n",
    "df['Datetime'] = pd.to_datetime(df['Datetime'])  # Adjust the 'datetime' column name as necessary\n",
    "df = df.sort_values('Datetime').reset_index(drop=True)\n",
    "\n",
    "\n",
    "# Obtain day and hour\n",
    "df['Date'] = pd.to_datetime(df['Datetime']).dt.floor('D')  \n",
    "df['Hour'] = pd.to_datetime(df['Datetime']).dt.hour\n",
    "\n",
    "# Create multi-index feature df to compute time series features on\n",
    "features = df.set_index(['Date', 'Hour'])  \n",
    "features.drop(\"Datetime\", inplace=True, axis=1)\n",
    "\n",
    "# Split the data into training and testing sets, respecting the temporal order\n",
    "X_train, X_test, y_train, y_test = train_test_split(features, features[\"PJME_MW\"], test_size=0.2, shuffle=False)\n",
    "\n",
    "# Get group lengths\n",
    "train_lengths = X_train.groupby(level=0).size()\n",
    "test_lengths = X_test.groupby(level=0).size()\n",
    "\n",
    "# Obtain common length value for train/test data\n",
    "train_common_length = train_lengths.mode().iloc[0]\n",
    "test_common_length = test_lengths.mode().iloc[0]\n",
    "\n",
    "# Filter train/test data to groups with same common length for featurizer\n",
    "X_train = X_train.groupby(level=0).filter(lambda x: len(x) == train_common_length)\n",
    "X_test = X_test.groupby(level=0).filter(lambda x: len(x) == test_common_length)\n",
    "\n",
    "# Create quartiles based on training data to avoid leakage\n",
    "quartiles = [X_train['PJME_MW'].quantile(q) for q in [0.25, 0.50, 0.75]]"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1068710b-9f76-4988-a43f-bb8a550c1d6d",
   "metadata": {},
   "source": [
    "## Train and Evaluate Prophet Forecasting Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcb619ec-f09d-4030-9f28-7a128e434f74",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Cutoff date at 2015-04-09\n",
    "cutoff_index = int(len(daily_data) * 0.8)\n",
    "\n",
    "# Use 80% of data for training set and 20% for test set\n",
    "train_df = daily_data.iloc[:cutoff_index]\n",
    "test_df = daily_data.iloc[cutoff_index:]\n",
    "\n",
    "print(\"Training Set Shape:\", train_df.shape)\n",
    "print(\"Testing Set Shape:\", test_df.shape)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3fd144e-9e93-4b7b-8808-991e52ea40b5",
   "metadata": {},
   "outputs": [],
   "source": [
    "train_df.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3fa64b01-ca61-4b8d-ab51-58cb8eb6e964",
   "metadata": {},
   "outputs": [],
   "source": [
    "test_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c71781e-7e82-407b-8200-a2ce3333ee64",
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "from prophet import Prophet\n",
    "from sklearn.metrics import accuracy_score\n",
    "\n",
    "# Initialize model and train it on training data\n",
    "model = Prophet()\n",
    "model.fit(train_df)\n",
    "\n",
    "# Create a dataframe for future predictions covering the test period\n",
    "future = model.make_future_dataframe(periods=len(test_df), freq='D')\n",
    "forecast = model.predict(future)\n",
    "\n",
    "# Categorize forecasted daily values into quartiles based on the thresholds\n",
    "forecast['quartile'] = pd.cut(forecast['yhat'], bins = [-np.inf] + list(quartiles) + [np.inf], labels=[1, 2, 3, 4])\n",
    "\n",
    "# Extract the forecasted quartiles for the test period\n",
    "forecasted_quartiles = forecast.iloc[-len(test_df):]['quartile'].astype(int)\n",
    "\n",
    "\n",
    "# Categorize actual daily values in the test set into quartiles\n",
    "test_df['quartile'] = pd.cut(test_df['y'], bins=[-np.inf] + list(quartiles) + [np.inf], labels=[1, 2, 3, 4])\n",
    "actual_test_quartiles = test_df['quartile'].astype(int)\n",
    "\n",
    "\n",
    "# Calculate the evaluation metrics\n",
    "accuracy = accuracy_score(actual_test_quartiles, forecasted_quartiles)\n",
    "\n",
    "# Print the evaluation metrics\n",
    "print(f'Accuracy: {accuracy:.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b039e99-2ac9-4a36-85b9-f874a5947144",
   "metadata": {},
   "source": [
    "## Convert time series data to tabular format through featurization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4755933-2bc5-4f18-bb9f-d07769075603",
   "metadata": {},
   "outputs": [],
   "source": [
    "import tsfel\n",
    "from sktime.transformations.panel.tsfresh import TSFreshFeatureExtractor\n",
    "\n",
    "# Define tsfresh feature extractor\n",
    "tsfresh_trafo = TSFreshFeatureExtractor(default_fc_parameters=\"minimal\")\n",
    "\n",
    "# Transform the training data using the feature extractor\n",
    "X_train_transformed = tsfresh_trafo.fit_transform(X_train)\n",
    "\n",
    "# Transform the test data using the same feature extractor\n",
    "X_test_transformed = tsfresh_trafo.transform(X_test)\n",
    "\n",
    "# Retrieves a pre-defined feature configuration file to extract all available features\n",
    "cfg = tsfel.get_features_by_domain()\n",
    "\n",
    "# Function to compute tsfel features per day\n",
    "def compute_features(group):\n",
    "    # TSFEL expects a DataFrame with the data in columns, so we transpose the input group\n",
    "    features = tsfel.time_series_features_extractor(cfg, group, fs=1, verbose=0)\n",
    "    return features\n",
    "\n",
    "\n",
    "# Group by the 'day' level of the index and apply the feature computation\n",
    "train_features_per_day = X_train.groupby(level='Date').apply(compute_features).reset_index(drop=True)\n",
    "test_features_per_day = X_test.groupby(level='Date').apply(compute_features).reset_index(drop=True)\n",
    "\n",
    "# Combine each featurization into a set of combined features for our train/test data\n",
    "train_combined_df = pd.concat([X_train_transformed, train_features_per_day], axis=1)\n",
    "test_combined_df = pd.concat([X_test_transformed, test_features_per_day], axis=1)\n",
    "\n",
    "# Filter out features that are highly correlated with our target variable\n",
    "column_of_interest = \"PJME_MW__mean\"\n",
    "train_corr_matrix = train_combined_df.corr()\n",
    "train_corr_with_interest = train_corr_matrix[column_of_interest]\n",
    "null_corrs = pd.Series(train_corr_with_interest.isnull())\n",
    "false_features = null_corrs[null_corrs].index.tolist()\n",
    "\n",
    "columns_to_exclude = list(set(train_corr_with_interest[abs(train_corr_with_interest) > 0.8].index.tolist() + false_features))\n",
    "columns_to_exclude.remove(column_of_interest)\n",
    "\n",
    "# Filtered DataFrame excluding columns with high correlation to the column of interest\n",
    "train_combined_df = train_combined_df.drop(columns=columns_to_exclude)\n",
    "test_combined_df = test_combined_df.drop(columns=columns_to_exclude)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4d50faef-1dcd-4900-8fe2-15001ccd4efc",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Define a function to classify each value into a quartile\n",
    "def classify_into_quartile(value):\n",
    "    if value < quartiles[0]:\n",
    "        return 1  \n",
    "    elif value < quartiles[1]:\n",
    "        return 2  \n",
    "    elif value < quartiles[2]:\n",
    "        return 3  \n",
    "    else:\n",
    "        return 4  \n",
    "\n",
    "X_train_transformed = train_combined_df.copy()\n",
    "X_test_transformed = test_combined_df.copy()\n",
    "\n",
    "y_train = X_train_transformed[\"PJME_MW__mean\"]\n",
    "X_train_transformed.drop(\"PJME_MW__mean\", inplace=True, axis=1)\n",
    "\n",
    "y_test = X_test_transformed[\"PJME_MW__mean\"]\n",
    "X_test_transformed.drop(\"PJME_MW__mean\", inplace=True, axis=1)\n",
    "\n",
    "y_train_labels = y_train.apply(classify_into_quartile)\n",
    "y_test_labels = y_test.apply(classify_into_quartile)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54236393-8463-4c83-8cc0-c28b26db2a5a",
   "metadata": {},
   "source": [
    "## Train and Evaluate GradientBoostingClassifier Model on multiclass tabular data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a34605d9-f596-44e3-a379-2b20febd9d87",
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.ensemble import GradientBoostingClassifier\n",
    "\n",
    "gbc = GradientBoostingClassifier(\n",
    "    n_estimators=150,\n",
    "    learning_rate=0.1,\n",
    "    max_depth=4,\n",
    "    min_samples_leaf=20,\n",
    "    max_features='sqrt',\n",
    "    subsample=0.8,\n",
    "    random_state=42\n",
    ")\n",
    "\n",
    "gbc.fit(X_train_transformed, y_train_labels)\n",
    "\n",
    "\n",
    "y_pred_gbc = gbc.predict(X_test_transformed)\n",
    "print(f'Accuracy: {accuracy_score(y_test_labels, y_pred_gbc):.4f}')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f210a381-3159-4891-8cd3-de658d77c06f",
   "metadata": {},
   "source": [
    "## Read in AutoML pred probs to compare measure out-of-sample accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e7d67b0e-6995-4667-accf-c49ab65c7042",
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred_automl_cleanlab = pd.read_csv(\"quartile-multiclass-pjme-testing-data_pred_probs.csv\")\n",
    "y_pred_automl_cleanlab = y_pred_automl_cleanlab[\"Suggested Label\"]\n",
    "print(f'Accuracy: {accuracy_score(y_test_labels, y_pred_automl_cleanlab):.4f}')"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "time-series-automl-env",
   "language": "python",
   "name": "time-series-automl-env"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
